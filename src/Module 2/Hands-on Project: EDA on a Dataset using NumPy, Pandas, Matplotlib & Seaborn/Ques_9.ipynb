{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"18_5NIBA2DcMfzp5oMt26VHboka2jAi1Y","authorship_tag":"ABX9TyM/nDjZBQeN6POHyhORp2B/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import warnings\n","\n","# Sample data (simulated flower data for classification)\n","data = {\n","    'sepal_length': [5.1, 4.9, 7.0, 6.4, 5.8, 6.3, 5.0, 6.5, 6.2, 5.9, 5.0, 7.7, 6.0, 6.9, 5.6, 6.7, 6.3, 5.7, 6.4, 5.5],\n","    'sepal_width': [3.5, 3.0, 3.2, 3.2, 2.7, 3.3, 3.6, 2.8, 2.2, 3.2, 2.3, 3.8, 2.9, 3.1, 3.0, 2.5, 2.8, 2.8, 3.1, 2.4],\n","    'petal_length': [1.4, 1.4, 4.7, 4.5, 5.1, 6.0, 1.4, 4.6, 4.5, 4.8, 3.3, 6.7, 4.0, 4.9, 3.9, 5.7, 5.1, 4.5, 5.3, 3.8],\n","    'petal_width': [0.2, 0.2, 1.3, 1.5, 1.9, 2.5, 0.2, 1.5, 1.5, 1.8, 1.0, 2.0, 1.2, 1.5, 1.1, 2.1, 1.8, 1.3, 1.9, 1.1],\n","    'species': ['Setosa', 'Setosa', 'Versicolor', 'Versicolor', 'Virginica', 'Virginica', 'Setosa', 'Versicolor', 'Versicolor', 'Virginica', 'Setosa', 'Virginica', 'Versicolor', 'Virginica', 'Versicolor', 'Virginica', 'Virginica', 'Versicolor', 'Virginica', 'Versicolor']\n","}\n","df = pd.DataFrame(data)\n","\n","# 1. Feature Engineering: Create a new feature\n","#   - Assumption:  Setosa species has smaller petal area than others.\n","#   - New Feature: 'petal_area' = petal_length * petal_width\n","def create_petal_area(df):\n","    \"\"\"\n","    Creates a new feature 'petal_area' in the DataFrame.\n","\n","    Args:\n","        df (pd.DataFrame): Input DataFrame containing 'petal_length' and 'petal_width'.\n","\n","    Returns:\n","        pd.DataFrame: DataFrame with the added 'petal_area' feature.\n","    \"\"\"\n","    df['petal_area'] = df['petal_length'] * df['petal_width']\n","    return df\n","\n","# 2. Model Training and Evaluation\n","def train_and_evaluate_model(X_train, y_train, X_test, y_test, model_name=\"Decision Tree\"):\n","    \"\"\"\n","    Trains a classification model and evaluates its performance.\n","\n","    Args:\n","        X_train (pd.DataFrame): Training features.\n","        y_train (pd.Series): Training target variable.\n","        X_test (pd.DataFrame): Testing features.\n","        y_test (pd.Series): Testing target variable.\n","        model_name (str, optional): Name of the model for reporting. Defaults to \"Decision Tree\".\n","\n","    Returns:\n","        tuple: (accuracy, classification_report) of the model on the test set.\n","    \"\"\"\n","    model = DecisionTreeClassifier(random_state=42)  # Initialize Decision Tree model\n","    model.fit(X_train, y_train)             # Train the model\n","    y_pred = model.predict(X_test)    # Make predictions\n","    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n","    report = classification_report(y_test, y_pred)    # Get detailed classification report\n","\n","    print(f\"\\nResults for {model_name}:\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(report)\n","\n","    return accuracy, report\n","\n","# 3. Feature Importance\n","def display_feature_importance(model, X):\n","    \"\"\"\n","    Displays the feature importance from the trained model.\n","\n","    Args:\n","        model: Trained classification model (must have feature_importances_ attribute).\n","        X (pd.DataFrame): The feature DataFrame used for training.\n","    \"\"\"\n","    if hasattr(model, 'feature_importances_'):\n","        print(\"\\nFeature Importances:\")\n","        print(model.feature_importances_)  # Print the importance of each feature\n","\n","        feature_names = X.columns\n","        importances = model.feature_importances_\n","        indices = np.argsort(importances)[::-1]  # Sort feature importances in descending order\n","\n","        # Print the feature ranking\n","        print(\"Feature ranking:\")\n","        for f in range(X.shape[1]):\n","            print(f\"{f+1}. {feature_names[indices[f]]} ({importances[indices[f]]:.3f})\")\n","    else:\n","        print(f\"\\nFeature Importance not available for model type: {type(model).__name__}\")\n","\n","def main():\n","    \"\"\"\n","    Main function to orchestrate the feature engineering, model training, and evaluation.\n","    \"\"\"\n","    # Load and prepare data\n","    df = pd.DataFrame(data)\n","    df = create_petal_area(df) # Create the petal_area feature\n","\n","    # Prepare data for classification\n","    X = df.drop('species', axis=1)  # Features (including new 'petal_area')\n","    y = df['species']             # Target variable ('species' column)\n","\n","    #hold out 20% of the data for final testing\n","    X_train_val, X_test_final, y_train_val, y_test_final = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # 2. Verify Utility of the New Feature with a simple classification model\n","    #   - Use Decision Tree (simple, interpretable)\n","    #   - Compare performance with and without the new feature\n","\n","    # 2.a: Train and Evaluate WITHOUT the new feature\n","    X_train_no_area = X_train_val.drop('petal_area', axis=1)\n","    X_test_no_area  = X_test_final.drop('petal_area', axis=1)\n","\n","    accuracy_no_area, report_no_area = train_and_evaluate_model(X_train_no_area, y_train_val, X_test_no_area, y_test_final, model_name=\"Decision Tree (No Petal Area)\")\n","\n","    # 2.b: Train and Evaluate WITH the new feature ('petal_area')\n","    accuracy_with_area, report_with_area = train_and_evaluate_model(X_train_val, y_train_val, X_test_final, y_test_final, model_name=\"Decision Tree (With Petal Area)\")\n","\n","    # 3. Feature Importance\n","    #  Display feature importances from the model trained with 'petal_area'\n","    model_with_area = DecisionTreeClassifier(random_state=42)\n","    model_with_area.fit(X_train_val, y_train_val)\n","    display_feature_importance(model_with_area, X_train_val)\n","\n","    # 4. Model Stability Check\n","    print(\"\\nModel Stability Check (Multiple Train/Test Splits):\")\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5-fold cross-validation\n","    accuracies_with_area = []\n","    accuracies_no_area = []\n","\n","    for train_index, val_index in kf.split(X_train_val):\n","        X_train_fold, X_val_fold = X_train_val.iloc[train_index], X_train_val.iloc[val_index]\n","        y_train_fold, y_val_fold = y_train_val.iloc[train_index], y_train_val.iloc[val_index]\n","\n","        # With petal area\n","        model_with_area_fold = DecisionTreeClassifier(random_state=42)\n","        model_with_area_fold.fit(X_train_fold, y_train_fold)\n","        acc_with_area_fold = accuracy_score(y_val_fold, model_with_area_fold.predict(X_val_fold))\n","        accuracies_with_area.append(acc_with_area_fold)\n","\n","        # Without petal area\n","        model_no_area_fold = DecisionTreeClassifier(random_state=42)\n","        model_no_area_fold.fit(X_train_fold.drop('petal_area',axis=1), y_train_fold)\n","        acc_no_area_fold = accuracy_score(y_val_fold, model_no_area_fold.predict(X_val_fold.drop('petal_area',axis=1)))\n","        accuracies_no_area.append(acc_no_area_fold)\n","\n","    print(f\"Average Accuracy with petal_area: {np.mean(accuracies_with_area):.4f}, Std Dev: {np.std(accuracies_with_area):.4f}\")\n","    print(f\"Average Accuracy without petal_area: {np.mean(accuracies_no_area):.4f}, Std Dev: {np.std(accuracies_no_area):.4f}\")\n","\n","    # Final evaluation on the held-out test set\n","    print(\"\\nFinal Evaluation on Held-Out Test Set:\")\n","    train_and_evaluate_model(X_train_val, y_train_val, X_test_final, y_test_final, model_name=\"Decision Tree (Final Test)\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","# Title: Feature Engineering for Classification\n","# Description: Create a new feature that could help distinguish between species based on\n","# logical assumptions and verify its utility.\n"],"metadata":{"id":"aBY1jFZeUXc1","executionInfo":{"status":"ok","timestamp":1745825664716,"user_tz":-330,"elapsed":621,"user":{"displayName":"Rachana S Shekar","userId":"15964282208917255786"}},"outputId":"874acba0-fba3-428d-c667-f6e18a216a66","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Results for Decision Tree (No Petal Area):\n","Accuracy: 1.0000\n","              precision    recall  f1-score   support\n","\n","      Setosa       1.00      1.00      1.00         2\n","  Versicolor       1.00      1.00      1.00         1\n","   Virginica       1.00      1.00      1.00         1\n","\n","    accuracy                           1.00         4\n","   macro avg       1.00      1.00      1.00         4\n","weighted avg       1.00      1.00      1.00         4\n","\n","\n","Results for Decision Tree (With Petal Area):\n","Accuracy: 1.0000\n","              precision    recall  f1-score   support\n","\n","      Setosa       1.00      1.00      1.00         2\n","  Versicolor       1.00      1.00      1.00         1\n","   Virginica       1.00      1.00      1.00         1\n","\n","    accuracy                           1.00         4\n","   macro avg       1.00      1.00      1.00         4\n","weighted avg       1.00      1.00      1.00         4\n","\n","\n","Feature Importances:\n","[0.         0.         0.32323232 0.         0.67676768]\n","Feature ranking:\n","1. petal_area (0.677)\n","2. petal_length (0.323)\n","3. petal_width (0.000)\n","4. sepal_width (0.000)\n","5. sepal_length (0.000)\n","\n","Model Stability Check (Multiple Train/Test Splits):\n","Average Accuracy with petal_area: 0.8667, Std Dev: 0.1633\n","Average Accuracy without petal_area: 1.0000, Std Dev: 0.0000\n","\n","Final Evaluation on Held-Out Test Set:\n","\n","Results for Decision Tree (Final Test):\n","Accuracy: 1.0000\n","              precision    recall  f1-score   support\n","\n","      Setosa       1.00      1.00      1.00         2\n","  Versicolor       1.00      1.00      1.00         1\n","   Virginica       1.00      1.00      1.00         1\n","\n","    accuracy                           1.00         4\n","   macro avg       1.00      1.00      1.00         4\n","weighted avg       1.00      1.00      1.00         4\n","\n"]}]}]}